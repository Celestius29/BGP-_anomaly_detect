#Remember to preprocess your dataset and change values to int type.
import pandas as pd
dataset=pd.read_csv('Dataset.csv') #Change to your dataset name
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

X = dataset.iloc[:, 0:21].values
y = dataset.iloc[:, 21].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt')
model.fit(X_train, y_train)
features = pd.DataFrame({'feature': list(dataset.columns)[:-1], 'importance': model.feature_importances_}).sort_values(by="importance", ascending=False)
features
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

try:
  features = dataset.sort_values(by='importance', ascending=False)
except KeyError:
  print('KeyError: importance')
  exit(1)

# Draw a histogram for the feature importance
plt.bar(features['feature'], features['importance'], label='Importance')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance Histogram')

# Rotate the x axis labels to make them visible
plt.xticks(rotation=45, ha='right')

# Set the x axis tick labels to minor ticks
plt.gca().xaxis.set_minor_locator(mtick.MultipleLocator(1))
plt.gca().xaxis.set_minor_formatter(mtick.FormatStrFormatter('%s'))

# Display the minor ticks
plt.tick_params(which='both', bottom=True, top=False)

plt.legend()
plt.show()
from sklearn.model_selection import train_test_split

X = dataset.iloc[:, 0:21].values
y = dataset.iloc[:, 21].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt')
model.fit(X_train, y_train)
rf_predictions = model.predict(X_test)
rf_probs = model.predict_proba(X_test)[:, 1]


from sklearn.metrics import roc_auc_score
roc_auc = roc_auc_score(y_test, rf_probs)
roc_auc
import numpy as np
from sklearn.metrics import confusion_matrix
# Calculate TN, TP, FP, and FN
confusion_matrix = confusion_matrix(y_test, rf_predictions)

TN = confusion_matrix[0][0]
TP = confusion_matrix[1][1]
FP = confusion_matrix[0][1]
FN = confusion_matrix[1][0]

print("TN:", TN)
print("TP:", TP)
print("FP:", FP)
print("FN:", FN)
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from matplotlib.colors import ListedColormap

# Create a PCA object
pca = PCA(n_components=2)

# Fit the PCA object to the training data
pca.fit(X_train)

# Transform the training and test data using the PCA object
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# Create two color palettes
cmap_train = ListedColormap(['#FF0000'])
cmap_test = ListedColormap(['#00FFFF'])

# Create a scatter plot of the transformed training and test data
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, label='Training data', cmap=cmap_train, alpha=0.8)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, label='Test data', cmap=cmap_test, alpha=0.8)

# Additional customizations
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend()
plt.title('PCA graph with data separation')
plt.grid(True)
plt.show()
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy = accuracy_score(y_test, rf_predictions)
precision = precision_score(y_test, rf_predictions)
recall = recall_score(y_test, rf_predictions)
f1 = f1_score(y_test, rf_predictions)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

import xgboost as xgb
from sklearn.model_selection import train_test_split

X = dataset.iloc[:, 0:21].values
y = dataset.iloc[:, 21].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Create XGBoost classifier
xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, gamma=0.5)

# Train the XGBoost classifier
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
xgb_predictions = xgb_model.predict(X_test)

# Compute the ROC AUC score
from sklearn.metrics import roc_auc_score
xgb_roc_auc = roc_auc_score(y_test, xgb_predictions)

print("XGBoost ROC AUC score:", xgb_roc_auc)

import numpy as np
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer

# Load the dataset
dataset = pd.read_csv('dataset.csv')

# Impute the NaN values
imputer = SimpleImputer(strategy='mean')
imputed_dataset = imputer.fit_transform(dataset)

# Create a PCA object
pca = PCA(n_components=2)

# Fit the PCA model to the imputed data
pca.fit(imputed_dataset)

# Transform the data using the PCA model
X_pca = pca.transform(imputed_dataset)


# Compute the confusion matrix
confusion_matrix = confusion_matrix(y_test, xgb_predictions)

# Extract the TN, TP, FP, and FN values from the confusion matrix
TN = confusion_matrix[0][0]
TP = confusion_matrix[1][1]
FP = confusion_matrix[0][1]
FN = confusion_matrix[1][0]

# Print the TN, TP, FP, and FN values
print("TN:", TN)
print("TP:", TP)
print("FP:", FP)
print("FN:", FN)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, xgb_predictions)
precision = precision_score(y_test, xgb_predictions)
recall = recall_score(y_test, xgb_predictions)
f1 = f1_score(y_test, xgb_predictions)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

# Assuming you already have the ROC curves for both classifiers

# Plotting the ROC curve for Random Forest Classifier
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='Random Forest (AUC = %0.2f)' % roc_auc)

# Plotting the ROC curve for XGBoost Classifier
plt.plot(fpr_xgb, tpr_xgb, label='XGBoost (AUC = %0.2f)' % xgb_roc_auc)

# Adding labels and legend
plt.title('ROC Curves for Random Forest and XGBoost Classifiers')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.legend()

# Display the plot
plt.show()
